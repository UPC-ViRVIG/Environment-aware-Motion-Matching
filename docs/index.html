<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Environment-aware Motion Matching - Jose Luis Ponton, Sheldon Andrews, Carlos Andujar, Nuria Pelechano">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="We present Environment-aware Motion Matching, a novel real-time system for full-body character animation that dynamically adapts to obstacles and other agents, emphasizing the bidirectional relationship between pose and trajectory.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="motion matching, motion capture, character controller, character animation, siggraph asia 2025, machine learning">
  <!-- TODO: List all authors -->
  <meta name="author" content="Jose Luis Ponton, Sheldon Andrews, Carlos Andujar, Nuria Pelechano">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Universitat Politecnica de Catalunya">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Environment-aware Motion Matching">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="We present Environment-aware Motion Matching, a novel real-time system for full-body character animation that dynamically adapts to obstacles and other agents, emphasizing the bidirectional relationship between pose and trajectory.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://upc-virvig.github.io/Environment-aware-Motion-Matching/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="static/images/RepresentativeImage.jpg">
  <meta property="og:image:width" content="1500">
  <meta property="og:image:height" content="1000">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="Environment-aware Motion Matching">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="We present Environment-aware Motion Matching, a novel real-time system for full-body character animation that dynamically adapts to obstacles and other agents, emphasizing the bidirectional relationship between pose and trajectory.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="static/images/RepresentativeImage.jpg">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Environment-aware Motion Matching - Jose Luis Ponton, Sheldon Andrews, Carlos Andujar, Nuria Pelechano | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: Replace with your lab's related works -->
        <a href="https://upc-virvig.github.io/MMVR/" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Combining Motion Matching and Orientation Prediction to Animate Avatars for Consumer-Grade VR Devices</h5>
            <p>Motion Matching for Virtual Reality Avatars.</p>
            <span class="work-venue">SCA 2022</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://upc-virvig.github.io/DragPoser/" class="work-item" target="_blank">
          <div class="work-info">
            <!-- TODO: Replace with actual paper title -->
            <h5>DragPoser: Motion Reconstruction from Variable Sparse Tracking Signals via Latent Space Optimization</h5>
            <!-- TODO: Replace with brief description -->
            <p>We introduce DragPoser, a novel deep-learning-based motion reconstruction system that accurately represents hard and dynamic constraints, attaining real-time high end-effectors position accuracy.</p>
            <!-- TODO: Replace with venue and year -->
            <span class="work-venue">Eurographics 2025</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <!-- TODO: Add more related works or remove extra items -->
        <a href="https://upc-virvig.github.io/SparsePoser/" class="work-item" target="_blank">
          <div class="work-info">
            <h5>SparsePoser: Real-Time Full-Body Motion Reconstruction from Sparse Data</h5>
            <p>SparsePoser is a novel deep learning-based approach for reconstructing full-body human motion in VR using a reduced set of six tracking devices.</p>
            <span class="work-venue">SIGGRAPH Asia 2023 / TOG</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Environment-aware Motion Matching</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://joseluisponton.com/" target="_blank">Jose Luis Ponton<sup>1</sup></a>,</span>
                <span class="author-block">
                  <a href="https://profs.etsmtl.ca/sandrews/" target="_blank">Sheldon Andrews<sup>2</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://www.cs.upc.edu/~virtual/home/index.html" target="_blank">Carlos Andujar<sup>1</sup></a>,</span>
                    <span class="author-block">
                      <a href="https://www.cs.upc.edu/~npelechano/" target="_blank">Nuria Pelechano<sup>1</sup></a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block">
                      <sup>1</sup>Universitat Politecnica de Catalunya, Spain<br>
                      <sup>2</sup>École de technologie supérieure, Canada<br>
                      SIGGRAPH Asia 2025 | ACM TOG
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://doi.org/10.1145/3763334" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/UPC-ViRVIG/Environment-aware-Motion-Matching" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- TODO: Replace with your teaser image -->
      <img src="static/images/EMM_Teaser.png" alt="Environment-aware Motion Matching teaser showing characters adapting their motion to navigate complex environments" loading="lazy" style="width: 100%; height: auto;"/>
      <!-- TODO: Replace with your image description -->
      <h2 class="subtitle has-text-centered">
        Our real-time system enables characters to dynamically adapt their full-body pose and trajectory to navigate complex environments and interact with obstacles and other agents, seamlessly blending motion capture data with environmental constraints.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Traditional character animation techniques often struggle to handle arbitrary situations, leading to a growing trend of dynamically selecting motion-captured animations based on predefined features. While Motion Matching has proven effective for locomotion by aligning to target trajectories, animating environment interactions and crowd behaviors remains challenging due to the need to consider surrounding elements. Existing approaches often involve manual setup or lack the naturalism of motion capture. Furthermore, in crowd animation, body animation is frequently treated as a separate process from trajectory planning, leading to inconsistencies between body pose and root motion. To address these limitations, we present Environment-aware Motion Matching, a novel real-time system for full-body character animation that dynamically adapts to obstacles and other agents, emphasizing the bidirectional relationship between pose and trajectory. In a preprocessing step, we extract shape, pose, and trajectory features from a motion capture database. At runtime, we perform an efficient search that matches user input and current pose while penalizing collisions with a dynamic environment. Our method allows characters to naturally adjust their pose and trajectory to navigate crowded scenes.   
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- TODO: Replace with your YouTube video ID -->
            <iframe src="https://www.youtube.com/embed/u8LISEkGsyk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<!-- Method Overview Section -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
  <div class="column is-four-fifths">
        <div class="content">
          <h2 class="title is-3">Method Overview</h2>
          <center>
            <img src="static/images/mainpipeline.png" alt="Main Pipeline" class="center-image" loading="lazy"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Our system operates in two distinct stages: a preprocessing phase and a real-time controller. In preprocessing, we extract pose, trajectory, and environment features from motion capture data. At runtime, the real-time controller constructs a query vector from user input and the current character pose, which is compared against query features. Simultaneously, environment features guide the search by computing dynamic obstacle penalizations. Pose and trajectory features are defined similarly to existing Motion Matching methods, while our novel environment features enable environment-aware pose search. Query features are compared to target values, whereas environment features are used to compute penalization factors based on dynamic scene analysis.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Video Results Overview Section -->
<section class="section is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <div class="content">
          <h2 class="title is-3">Results</h2>
          <div class="level-set has-text-justified">
            <p>
              Below are several video demonstrations showcasing our system in different scenarios. Each video highlights a unique aspect of environment-aware motion matching, including navigation in narrow corridors, zigzag movement, and multi-character interactions.
            </p>
          </div>
          <div class="video-demo-block">
            <h3 class="title is-5" style="margin-top:2.5em;">Narrow Corridor Navigation</h3>
            <center>
              <video poster="" controls muted loop preload="metadata" style="width:100%;max-width:600px;">
                <source src="static/videos/Fig12_NarrowCorridor.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </center>
            <div class="level-set has-text-justified">
              <p>
                Character navigating a progressively narrowing corridor. As the corridor narrows to 0.35 meters, the character reduces speed and transitions to side-stepping. When the corridor becomes wider again, the system switches back to a running animation. This demonstrates the system's ability to adapt body pose and speed to tight environmental constraints.
              </p>
            </div>
          </div>
          <div class="video-demo-block">
            <h3 class="title is-5" style="margin-top:2.5em;">Zigzag Path Following</h3>
            <center>
              <video poster="" controls muted loop preload="metadata" style="width:100%;max-width:600px;">
                <source src="static/videos/Fig13_Zigzag.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </center>
            <div class="level-set has-text-justified">
              <p>
                Character traversing a corridor with zigzag cone obstacles. Despite receiving only a continuous forward input, our system enables the character to dynamically zigzag, maintain velocity, and transition to side-stepping through narrower sections or partially closed doorways. This illustrates the generation of complex trajectories from simple user commands due to environment awareness.
              </p>
            </div>
          </div>
          <div class="video-demo-block">
            <h3 class="title is-5" style="margin-top:2.5em;">Multi-Character Corridor Interaction</h3>
            <center>
              <video poster="" controls muted loop preload="metadata" style="width:100%;max-width:600px;">
                <source src="static/videos/Fig16_Multi_Character_Corridor.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </center>
            <div class="level-set has-text-justified">
              <p>
                Multi-character interaction in corridors of varying widths. Blue corridor (1.55m width): Characters walk with minimal body adjustment. Green corridor (1.20m width): Characters exhibit body turns to pass each other while walking. Red corridor (0.95m width): Characters carefully avoid each other. Two examples of the red corridor are provided to demonstrate the diversity of poses generated by our method.
              </p>
            </div>
          </div>
          <div class="video-demo-block">
            <h3 class="title is-5" style="margin-top:2.5em;">Adapting to Moving Cubes</h3>
            <center>
              <video poster="" controls muted loop preload="metadata" style="width:100%;max-width:600px;">
                <source src="static/videos/Fig14_MovingCubes.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </center>
            <div class="level-set has-text-justified">
              <p>
                Character adapting to two moving cubes with no user input. The character, initially stationary, automatically takes a few steps forward to avoid colliding with the approaching cubes. This demonstrates the system's real-time adaptation to dynamic obstacles without user intervention.
              </p>
            </div>
          </div>
            <div class="video-demo-block">
              <h3 class="title is-5" style="margin-top:2.5em;">Avoiding a Moving Car</h3>
              <center>
                <video poster="" controls muted loop preload="metadata" style="width:100%;max-width:600px;">
                  <source src="static/videos/Fig15_MovingCar.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </center>
              <div class="level-set has-text-justified">
                <p>
                  Character avoiding a moving car despite conflicting user input. The user inputs a forward movement (towards the car). However, as the car moves backward, the system makes the character naturally transition to a backward jogging animation, adapting pose and root motion to avoid collision.
                </p>
              </div>
            </div>
              <div class="video-demo-block">
                <h3 class="title is-5" style="margin-top:2.5em;">Agent Interaction: Different Speeds</h3>
                <center>
                  <video poster="" controls muted loop preload="metadata" style="width:100%;max-width:600px;">
                    <source src="static/videos/Fig17_Agent_Interaction.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </center>
                <div class="level-set has-text-justified">
                  <p>
                    Interaction between agents with different speeds. (1) A running character (red shirt) approaches a walking character, leading to a noticeable torso rotation for avoidance. (2) Both characters are walking, resulting in more subtle torso rotations for collision avoidance.
                  </p>
                </div>
              </div>
              <div class="video-demo-block">
                <h3 class="title is-5" style="margin-top:2.5em;">Jumping Over Obstacles</h3>
                <center>
                  <video poster="" controls muted loop preload="metadata" style="width:100%;max-width:600px;">
                    <source src="static/videos/Fig18_Jumping.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </center>
                <div class="level-set has-text-justified">
                  <p>
                    Character interacting with vertical obstacles using height features. The character jumps over a fence to maintain its target trajectory, demonstrating the system's ability to select appropriate vertical movements to overcome obstacles.
                  </p>
                </div>
              </div>
              <div class="video-demo-block">
                <h3 class="title is-5" style="margin-top:2.5em;">Crouching Under Ceilings</h3>
                <center>
                  <video poster="" controls muted loop preload="metadata" style="width:100%;max-width:600px;">
                    <source src="static/videos/Fig19_Crouching.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </center>
                <div class="level-set has-text-justified">
                  <p>
                    Character adapting to varying ceiling heights. The character progressively crouches to pass under a semitransparent red ceiling. When faced with a significantly lower ceiling, the character naturally transitions to a lying-down pose to traverse the obstacle, showcasing vertical adaptation.
                  </p>
                </div>
              </div>
              <div class="video-demo-block">
                <h3 class="title is-5" style="margin-top:2.5em;">Prop Weapon Adaptation</h3>
                <center>
                  <video poster="" controls muted loop preload="metadata" style="width:100%;max-width:600px;">
                    <source src="static/videos/Fig20_Prop.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </center>
                <div class="level-set has-text-justified">
                  <p>
                    Character adapting pose while holding a prop weapon. The character lowers the weapon as it navigates between columns to successfully fit through the narrow gaps.
                  </p>
                </div>
              </div>
              <div class="video-demo-block">
                <h3 class="title is-5" style="margin-top:2.5em;">Carrying a Large Box</h3>
                <center>
                  <video poster="" controls muted loop preload="metadata" style="width:100%;max-width:600px;">
                    <source src="static/videos/Fig21_Box.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </center>
                <div class="level-set has-text-justified">
                  <p>
                    Character carrying a large box. The character raises the box above its head to pass between columns.
                  </p>
                </div>
              </div>
              <div class="video-demo-block">
                <h3 class="title is-5" style="margin-top:2.5em;">Locomotion Style Adaptation</h3>
                <center>
                  <video poster="" controls muted loop preload="metadata" style="width:100%;max-width:600px;">
                    <source src="static/videos/Fig22_LocomotionStyle.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </center>
                <div class="level-set has-text-justified">
                  <p>
                    Example of a different locomotion style. A character typically walking with elbows raised adopts a more compact <i>quiet</i> pose when near other agents to fit through spaces, demonstrating adaptation to social or spatial constraints based on animation style.
                  </p>
                </div>
              </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video results overview section -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{2025:ponton:emm,
  author = {Ponton, Jose Luis and Andrews, Sheldon and Andujar, Carlos and Pelechano, Nuria},
  title = {Environment-aware Motion Matching},
  year = {2025},
  publisher = {Association for Computing Machinery},
  booktitle = {SIGGRAPH Asia 2025},
  address = {New York, NY, USA},
  issn = {0730-0301},
  doi = {10.1145/3763334},
  journal = {ACM Trans. Graph.},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
